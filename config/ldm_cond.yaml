model:
  unet:
    in_channels: 4
    out_channels: 4
    channels: 192
    n_res_blocks: 2
    attention_levels: [1, 2, 4, 8]   # 32, 16, 8, 4
    channel_multipliers: [1,2,2,4,4]  # 32, 16, 8, 4, 2
    n_heads: 8
    tf_layers: 1
    d_cond: 384

  encoder:
    channels: 128
    in_channels: 3
    out_channels: 3
    channel_multipliers: [1,2,4,4]  # num_down = len(ch_mult)-1
    num_resnet_blocks: 2
    dropout: 0.0
    resolution: 256
    z_channels: 4
    double_z: True

  decoder:
    channels: 128
    in_channels: 3
    out_channels: 3
    channel_multipliers: [1,2,4,4]  # num_down = len(ch_mult)-1
    num_resnet_blocks: 2
    dropout: 0.0
    resolution: 256
    z_channels: 4
    double_z: True

  auto_encoder:
    emb_channels: 4
    z_channels: 4

  ldm:
    latent_scaling_factor: 1
    n_steps: 1000
    linear_start: 0.0015
    linear_end: 0.0155

    context_embedder:
      model_name: 'intfloat/multilingual-e5-small'

training:
  use_cuda: true
  image_size: 256
  batch_size: 96
  epochs: 100
  learning_rate: 5.0e-5 
  weight_decay: 5.0e-5 
  model_path: './checkpoint/MS_COCO_cond_LDM.pkl'
  vae_model_path: './checkpoint/MS_COCO_VAE.pkl'
  load_model: false
